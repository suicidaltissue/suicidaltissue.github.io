---
type: Lecture Notes
panopto: 
lecturer: 
slides: https://blackboard.aber.ac.uk/ultra/courses/_46446_1/outline/edit/document/_2758228_1?courseId=_46446_1&view=content
notesCompleted: true
dateCompleted: 
module:
---
# Linear Regression


## Equation of a straight line
Believe it or not there is a mathematical equation to draw a straight line.

## Example
![[Pasted image 20240511150321.png]]

## Many Lines
![[Pasted image 20240511150332.png]]

## Linear Models
- We can find a line of best fit
- We use this as a 'linear model' of our data
- We can use this model to predict y from x or to forecast future y values
- We can evaluate the quality of the the model

## Methods to find line of best fit
There are a number of methods you can use to find the line of best fit
- Least Squares
- Theii-Sen

## Residuals
![[Pasted image 20240511150350.png]]
Residuals are the distances between the real y values and the line of best fit.
## Least Squares Method
• We want to find a line y = mx + c, so need m and c. 
• Line is chosen to minimise the sum of the squares of residuals (distances between y values and line). 
![[Pasted image 20240511150537.png]]
where y¯ and x¯ are means of y and x
### Equivalent Alternatives
![[Pasted image 20240511150558.png]]
### In Python
scipy.stats.linregress(x, y) 
Also see sklearn library and sklearn.linear_model.LinearRegression
#### Try It
```python
x = [9, 5, 10, 8, 7, 8, 15, 17, 3, 0, 0, 4, 10, 19, 4, 8, 12, 13, 17, 4, 5, 12, 19, 19, 0, 7, 18, 7, 19, 14]
y = [7, 6, 9, 10, 8, 8, 16, 17, 4, 0, 1, 7, 8, 17, 4, 3, 12, 16, 15, 1, 3, 15, 21, 20, 3, 9, 16, 9, 20, 15] 

# Plot the points 
plt.scatter(x, y) 

# Find and plot the line 
result = stats.linregress (x, y) 
predicted_y_vals = x∗result.slope + result.intercept
plt.plot(x, predicted_y_vals, color="red")
```

### Evaluating the model
![[Pasted image 20240511150821.png]]

#### Try It
```python
y_true = [100 , 50, 30, 20] 
y_pred = [90, 50, 50, 30] 
# calculate MAE, MSE, RMSE 
print(np.mean(np.abs(y_true - y_pred))) 
print(np.mean(( y_true - y_pred )∗∗2)) 
print(np.sqrt(np.mean(( y_true - y_pred )∗∗2)))
```
MAE is simple. MSE penalises larger errors. RMSE more interpretable than MSE?

##### Evaluating the Model
![[Pasted image 20240511151023.png]]
##### Warnings
Mean is distorted by outliers:
- Mean of [1,1,2,3,3,3,4,6,15] is 4.22
- Mean of [1,1,2,3,3,3,4,15,1000] is 114.7
So is variance, and standard deviation.
So is a least squares regression.
All of these rely on averaging/summing over all the data.
## Problem with outlier
![[Pasted image 20240511151214.png]]
An outlier in the middle can shift the line so it fits the data poorly.

![[Pasted image 20240511151245.png]]
An outlier at the end can pivot the line to be steeper/shallower than it should be.
## Robust Statistics
The median is much less affected by outliers than the mean. 
It doesn’t rely on averaging/summing all the data. It’s a robust statistic. 
Are there regression methods that are more robust, and use the median instead?
## Theil-Sen
Finds a robust line of best fit to your data using medians. 
How: Want to find a line y = mx + c 
- Find m using median. 
- Find c using median. 
This method first published in 1950 (Theil) and updated in 1968 (Sen).

### Find Median Slope
![[Pasted image 20240511151439.png]]
![[Pasted image 20240511151519.png]]
```python
#Or in Python:
scipy.stats.theilslopes(x,y)
```
### Better Fit When Outliers Present
![[Pasted image 20240511151626.png]]
Theil-Sen fits better than least squares when outliers present.

### Also Generally Useful
![[Pasted image 20240511151714.png]]
Theil-Sen fits well to noisy data too. More expensive to compute. Less obvious how to evaluate.
#lecturenote
