---
type: Lecture Notes
panopto: 
lecturer: 
slides: https://blackboard.aber.ac.uk/ultra/courses/_46446_1/outline/edit/document/_2759102_1?courseId=_46446_1&view=content
notesCompleted: true
dateCompleted: 
module:
---
# Data Reduction
Imagine we have a dataset of medical data about patients who have had surgery.

Some recover well and some need more care.
Why do some recover better?

We have a lot of data about each patient: age, smoking history,  
height, weight, current medication, other medical conditions,  
duration of illness, current symptoms, etc.  

Sometimes it can be difficult to get an overview of all this.

# Penguins Dataset
File palmerpenguins.csv (blackboard lecture 24)

Where to start data inspection? 2D plots are good but only let us see two attributes at a time.
There might be some redundancy. Maybe bill length and bill depth are correlated?
![[Pasted image 20240511144355.png]]
## Re-Describes Your Data
PCA provides a new way to describe/summarise your multidimensional data.
Re-express your data using new variables that are linear combinations of existing variables

Example: *bill_var = bill_depth + bill_length
		 penguin_size = body_mass + 0.5 * flipper_length*

We are aiming to find a smaller number of variables that:
- Avoid redundancy
- Show clear differences across your data items (penguins) so that we can perhaps use them to distinguish one species from another

For example if you have variables x1, x2, x3 (such as flipper length, bill length, body mass), 
PCA could transform your data into up to 3 new variables: 
- new1 = w1x1 + w2x2 + w3x3 
- new2 = w4x1 + w5x2 + w6x3 
- new3 = w7x1 + w8x2 + w9x3 

Some weights will be 0, some positive, some negative. We say these are “linear combinations” of your existing variables. 
PCA finds the best new linear combinations.
### How?
![[Pasted image 20240511144711.png]]
Gentoo bill length against bill depth. Looks like it’s correlated. Is there redundancy? Could we make one feature that summarises both without losing much?

![[Pasted image 20240511144752.png]]
Some linear combination of bill length and bill depth? 
w1bill_length + w2bill_depth for some numbers w1 and w2?
## Linear Regression
![[Pasted image 20240511144818.png]]
Least squares linear regression finds line that minimises residuals (errors from one variable). Instead we want a line that minimises error from both variables.
## Center Data
![[Pasted image 20240511144839.png]]
Centre the data around 0 by subtracting the mean from each of the variables. This allows us to find a line that goes through (0,0) that fits the data.
## Find Line
![[Pasted image 20240511144910.png]]
Find the line that minimises error (minimises perpendicular distances from points to line, rather than vertical distances). This also maximises variance along the line.
## In Python
```python
import pandas as pd 
import matplotlib.pyplot as plt 
from sklearn.decomposition import PCA 
from sklearn.preprocessing import StandardScaler

#read in data using pandas
penguins=pd.read_csv("palmerpenguins.csv", index_col="species")
penguins.dropna(inplace=True)
penguins.drop(columns=["island","sex"], inplace=True)

colors = {"Adelie":"blue",
		 "Gentoo":"orange","chinstrap":"green"}
```
### Plots
```python
# Plot two of the features 
plt.scatter(penguins["bill_length_mm"], 
			penguins["bill_depth_mm"], 
			c=penguins.index.map(colors )) 
plt.xlabel("bill length") 
plt.ylabel("bill depth") 
plt.show() 
plt.close()
```
### Normalize
If we don't do this, some of the features will have a bigger contribution than others.
```python
# Recentre and scale the data by mean and standard deviation 
scaler = StandardScaler() 
scaled_data = scaler.fit_transform(penguins) 
print(scaled_data)
```
### PCA Transformation
```python
# Do the PCA transformation 
pca = PCA() 
transformed_data = pca.fit_transform(scaled_data) 
# Plot the transformed data using the first two components
plt.scatter(transformed_data[:,0], transformed_data[:,1], 
			c=penguins.index.map(colors))
plt.show() 
plt.close()
```
### Weightings
```python
print("What are the weightings for each of the components?") 
print(pca.components_ [0:2 ,:])
```
The second principle component is:
0.09958812 0.00602395 - 0.03912818 0.10061775 - 0.98913648

That is, it's mostly representing "year" (weight of -0.98).
### Colouring by year
```python
# Do the PCA transformation 
pca = PCA() 
transformed_data = pca.fit_transform(scaled_data) # Plot the transformed data using the first two components
colors = {2007:'blue', 2008:'orange', 2009:'green'} 
plt.scatter(transformed_data[:,0], transformed_data[:,1],
			c=penguins["year"].map(colors))
plt.show() 
plt.close()
```
### Variance
```python
print(pca.explained_variance_ratio_)
```
We get:
0.55109649 0.20015151 0.15539186 0.07371312 0.01964703

So PC1 captures 55% of the variance present in the data. 
The first two components (PC1 and PC2) between them capture 75% of the variance. 
The first 3 components (PC1, PC2, PC3) capture over 90% of the variance. 

Could reduce your 5-variable dataset to 3 variables without much information loss?